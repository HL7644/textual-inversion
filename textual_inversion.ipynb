{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for data processing\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import PIL\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import einops\n",
    "import transformers\n",
    "import diffusers\n",
    "import accelerate\n",
    "#import clip\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "#from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "accelerator=accelerate.Accelerator()\n",
    "device=accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login using your token\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=\"<YOUR USERNAME>\"\n",
    "repo_name=\"<YOUR REPO NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resest tokenizer and text_encoder\n",
    "text_encoder=transformers.CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"text_encoder\")\n",
    "tokenizer=transformers.CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"tokenizer\")\n",
    "#save tokenizer and text_encoder to your repo\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "text_encoder.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt templates for training \n",
    "imagenet_templates_small = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "imagenet_style_templates_small = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textual Inversion Dataset\n",
    "class TextualInversionDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, tokenizer, text_encoder, images, initial_token, placeholder_token, learnable_property=\"object\", size=512, repeats=100, \n",
    "               interpolation=\"bicubic\", flip_p=0.5, center_crop=False):\n",
    "    super(TextualInversionDataset, self).__init__()\n",
    "    assert learnable_property in ['object', 'style'], 'Learnable Property should be either \"object\" or \"style\"'\n",
    "    #settings\n",
    "    self.learnable_property=learnable_property\n",
    "    self.size=size\n",
    "    self.repeats=repeats\n",
    "    self.flip_p=flip_p\n",
    "    self.center_crop=center_crop\n",
    "\n",
    "    #images preprocessing: assume input image is tensor\n",
    "    self.center_crop_transform=torchvision.transforms.CenterCrop(self.size)\n",
    "    self.image_transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(self.size, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=self.flip_p),\n",
    "    ])\n",
    "\n",
    "    #raw input to data\n",
    "    if len(images.size())==3:\n",
    "      images=images.unsqueeze(dim=0)\n",
    "    self.images=images\n",
    "    self.data=torch.cat([images for _ in range(repeats)], dim=0)\n",
    "    self.n_images=len(self.data)\n",
    "\n",
    "    #inversion tokens\n",
    "    self.initial_token=initial_token\n",
    "    self.placeholder_token=placeholder_token\n",
    "\n",
    "    #use CLIP tokenizer and text encoder\n",
    "    self.tokenizer=tokenizer\n",
    "    self.text_encoder=text_encoder\n",
    "    self.text_encoder=accelerator.prepare(self.text_encoder)\n",
    "\n",
    "    self.templates=imagenet_templates_small if self.learnable_property=='object' else imagenet_style_templates_small\n",
    "  \n",
    "  def __len__(self):\n",
    "    #consider repetition of the whole dataset\n",
    "    return self.n_images\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    example={}\n",
    "    #get image and text prompt\n",
    "    image=self.data[idx]\n",
    "    random_template=random.choice(self.templates).format(self.placeholder_token)\n",
    "    \n",
    "    #get prompt tokenized input ids\n",
    "    tokenized=self.tokenizer(random_template, padding=\"max_length\", truncation=True, max_length=self.tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "    input_ids, attention_mask=tokenized.input_ids.squeeze(dim=0), tokenized.attention_mask.squeeze(dim=0) #assumed that input ids is for a single prompt will be in size of (1,77)\n",
    "    \n",
    "    #get image pixel values\n",
    "    pixel_values=self.image_transforms(self.center_crop_transform(image)) if self.center_crop else self.image_transforms(image)\n",
    "\n",
    "    return input_ids, pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inversion Trainer\n",
    "class InversionTrainer():\n",
    "  def __init__(self):\n",
    "    #frozen models for inversion training\n",
    "    self.autoencoder=diffusers.AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"vae\")\n",
    "    self.cn_unet=diffusers.UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"unet\")\n",
    "\n",
    "    #save to my repo model to get updates\n",
    "    self.tokenizer=transformers.CLIPTokenizer.from_pretrained('{:s}/{:s}'.format(username, repo_name))\n",
    "\n",
    "    #train the pretrained one and replace the embeddings of my text_encoder with the trained encoder's embeddings\n",
    "    self.trainable_text_encoder=transformers.CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"text_encoder\")\n",
    "    self.text_encoder=transformers.CLIPTextModel.from_pretrained('{:s}/{:s}'.format(username, repo_name))\n",
    "\n",
    "    #DDPM scheduler to get timesteps & corruption during training.\n",
    "    self.train_scheduler=diffusers.DDPMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"scheduler\")\n",
    "\n",
    "    #accelerator prepare\n",
    "    self.autoencoder, self.cn_unet, self.text_encoder, self.trainable_text_encoder=accelerator.prepare(self.autoencoder, self.cn_unet, self.text_encoder, self.trainable_text_encoder)\n",
    "  \n",
    "  def freeze_module(self, module):\n",
    "    for param in module.parameters():\n",
    "      param.requires_grad=False\n",
    "    return\n",
    "  \n",
    "  def unfreeze_module(self, module):\n",
    "    for param in module.parameters():\n",
    "      param.requires_grad=True\n",
    "    return\n",
    "  \n",
    "  def freeze_all(self):\n",
    "    #freeze except token embeddings and inversion module\n",
    "    self.freeze_module(self.trainable_text_encoder.text_model.encoder)\n",
    "    self.freeze_module(self.trainable_text_encoder.text_model.embeddings.position_embedding)\n",
    "    self.freeze_module(self.trainable_text_encoder.text_model.final_layer_norm)\n",
    "\n",
    "    #freeze other modules\n",
    "    self.freeze_module(self.autoencoder)\n",
    "    self.freeze_module(self.cn_unet)\n",
    "    return\n",
    "  \n",
    "  def unfreeze_all(self):\n",
    "    self.unfreeze_module(self.trainable_text_encoder)\n",
    "    self.unfreeze_module(self.autoencoder)\n",
    "    self.unfreeze_module(self.cn_unet)\n",
    "    return\n",
    "  \n",
    "  def check_embedding_alignment(self):\n",
    "    #check embeddings of first 49408 embeddings\n",
    "    embeddings=self.text_encoder.text_model.embeddings.token_embedding.weight.data\n",
    "\n",
    "    #compared to pretrained text encoder\n",
    "    target_text_encoder=transformers.CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"text_encoder\").to(device)\n",
    "    target_embeddings=target_text_encoder.get_input_embeddings().weight.data\n",
    "\n",
    "    wrong_indices=[]\n",
    "    #check default 49408 embeddings.\n",
    "    for idx in range(49408):\n",
    "      emb=embeddings[idx]\n",
    "      target_emb=target_embeddings[idx]\n",
    "      if torch.equal(emb, target_emb)==False:\n",
    "        wrong_indices.append(idx)\n",
    "    if len(wrong_indices)>0:\n",
    "      print(\"{:d} default embeddings doesn't match\".format(len(wrong_indices)))\n",
    "      return False\n",
    "    else:\n",
    "      print(\"All default embeddings match\")\n",
    "      return True\n",
    "  \n",
    "  def plot_train_logs(self, train_logs):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    #plotting loss history\n",
    "    loss_history=train_logs['loss_history']\n",
    "    x=np.arange(1, len(loss_history)+1, 1)\n",
    "    plt.plot(x, loss_history)\n",
    "    plt.xlabel(\"Update Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "  def train(self, images, learnable_property, initial_token, placeholder_token, lr=5e-4, scale_lr=True, max_train_steps=2000, train_batch_size=4, gradient_checkpointing=True, seed=42, save=True):\n",
    "    #inherit hyperparams from Textual Inversion paper by default\n",
    "    #inversion training learns the CLIP text embeddings of the placeholder token representing the style image\n",
    "    inversion_train_logs={\n",
    "        'config': {\n",
    "            'lr': lr,\n",
    "            'scale_lr': scale_lr,\n",
    "            'max_train_steps': max_train_steps,\n",
    "            'train_batch_size': train_batch_size,\n",
    "            'gradient_accumulation_steps': accelerator.gradient_accumulation_steps,\n",
    "            'gradient_checkpointing': gradient_checkpointing,\n",
    "            'mixed_precision': accelerator.mixed_precision,\n",
    "            'seed': seed\n",
    "        },\n",
    "        'loss_history': [],\n",
    "    }\n",
    "\n",
    "    #expand images if input is a single image.\n",
    "    if len(images.size())==3:\n",
    "      images=images.unsqueeze(dim=0)\n",
    "\n",
    "    #update tokenizer\n",
    "    print(\"Initial Tokenizer Length: {:d}\".format(len(self.tokenizer)))\n",
    "    #add placeholder token and check that it doesn't exist inside the current dictionary.\n",
    "    num_added_tokens = self.tokenizer.add_tokens(placeholder_token)\n",
    "    print(\"No. of added tokens: {:d}\".format(num_added_tokens))\n",
    "    if num_added_tokens == 0:\n",
    "      raise ValueError(\n",
    "          f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n",
    "          \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "      )\n",
    "    placeholder_token_id = self.tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "    print(\"Placeholder Token: '{:s}', Placeholder Token ID: {:d}\".format(placeholder_token, placeholder_token_id))\n",
    "    \n",
    "    #expand the size of embeddings of text encoder\n",
    "    self.trainable_text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "    self.text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    #check initial token\n",
    "    if initial_token!=None:\n",
    "      token_ids = self.tokenizer.encode(initial_token, add_special_tokens=False)\n",
    "      # Check if initializer_token is a single token or a sequence of tokens\n",
    "      if len(token_ids) > 1:\n",
    "          raise ValueError(\"The initializer token must be a single token.\")\n",
    "      initial_token_id=token_ids[0]\n",
    "      print(\"Initial Token: '{:s}', Initial Token ID: {:d}\".format(initial_token, initial_token_id))\n",
    "      token_embeds = self.text_encoder.get_input_embeddings().weight.data\n",
    "      token_embeds[placeholder_token_id] = token_embeds[initial_token_id]\n",
    "\n",
    "    print(\"Final Tokenizer Length: {:d}\".format(len(self.tokenizer)))\n",
    "\n",
    "    #setup inversion dataset\n",
    "    inversion_dataset=TextualInversionDataset(self.tokenizer, self.trainable_text_encoder, images, initial_token, placeholder_token, learnable_property=learnable_property)\n",
    "\n",
    "    #set up train loaders and DDPM scheduler.\n",
    "    train_loader=torch.utils.data.DataLoader(inversion_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    #try optimizing only the placeholder embedding\n",
    "    optimizer=optim.AdamW(self.trainable_text_encoder.text_model.embeddings.token_embedding.parameters(), lr=lr)\n",
    "\n",
    "    #using huggingface accelerate\n",
    "    #1. Set up accelerator with gradient_accumulation_steps and mixed_precision using accelerate.Accelerator()\n",
    "    #2. enable/disable gradient checkpointing depending on available memory\n",
    "    #3. scale lr => to create effective lr from base_lr => learning_rate = (learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes)\n",
    "    #effective lr varies w.r.t effetive batch size. \n",
    "    #4. prepare using accelerator.prepare(models, optimizer, train_loader... etc)\n",
    "    #5. make prepared modules to have accelerator's mixed precision (ex) torch.float16)\n",
    "    #6. compute steps per epoch and total epochs => steps_per_epoch: len(train_loader)/gradient_accumulation_steps, num_train_epochs=max_train_steps/steps_per_epoch. (batch update steps)\n",
    "    #len(train_loader): # of batches inside a train_loader. \n",
    "    #7. during training loop: use with accelerator.accumulate(model) to use gradient accumulation\n",
    "\n",
    "    #print(accelerator.num_processes) #this is determined from gradient accumulation steps & GPU settings. => single GPU and single grad acc steps  => 1 process\n",
    "\n",
    "    #gradient checkpointing: method to reduce memory at a cost of more recomputations\n",
    "    if gradient_checkpointing:\n",
    "      self.trainable_text_encoder.gradient_checkpointing_enable()\n",
    "      self.cn_unet.enable_gradient_checkpointing()\n",
    "    \n",
    "    if scale_lr:\n",
    "      #effective learning rate => b.c. loss is averaged w.r.t batch size\n",
    "      lr = (lr * accelerator.gradient_accumulation_steps * train_batch_size * accelerator.num_processes)\n",
    "\n",
    "    #optimizer, train_data_loader subject to training\n",
    "    optimizer, train_loader = accelerator.prepare(optimizer, train_loader)\n",
    "    \n",
    "    #freeze all modules except text_encoder's token embeddings\n",
    "    self.freeze_all()\n",
    "\n",
    "    self.autoencoder.eval()\n",
    "    self.cn_unet.train()\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_loader) / accelerator.gradient_accumulation_steps)\n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "    total_batch_size = train_batch_size * accelerator.num_processes * accelerator.gradient_accumulation_steps\n",
    "    print(\"Dataset Size: {:d}, Update Steps per Epoch: {:d}, Train Epochs: {:d}\".format(len(inversion_dataset), num_update_steps_per_epoch, num_train_epochs))\n",
    "\n",
    "    mse_loss=nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    pbar=tqdm(desc=\"Textual Inversion Training\", total=max_train_steps)\n",
    "    global_step = 0\n",
    "    for epoch in range(1, num_train_epochs+1):\n",
    "      self.trainable_text_encoder.train()\n",
    "      for batch_step, batch_data in enumerate(train_loader):\n",
    "        with accelerator.accumulate(self.trainable_text_encoder):\n",
    "          input_ids, pixel_values=batch_data\n",
    "          #if global_step==0:\n",
    "          #  print(input_ids)\n",
    "          #print(input_ids.size(), attention_mask.size(), pixel_values.size())\n",
    "\n",
    "          #compute latents from pixel_values using autoencoder.encode\n",
    "          init_latents=self.autoencoder.encode(pixel_values).latent_dist.sample().detach() \n",
    "          init_latents=init_latents*0.18215 #z_0\n",
    "          eps=torch.randn_like(init_latents).to(device)\n",
    "          timesteps=torch.randint(low=0, high=self.train_scheduler.num_train_timesteps, size=[init_latents.size(0)]).long().to(device)\n",
    "          #obtaining z_t from z_0 through corruption.\n",
    "          noisy_latents = self.train_scheduler.add_noise(init_latents, eps, timesteps)\n",
    "\n",
    "          prompt_embeddings=self.trainable_text_encoder(input_ids).last_hidden_state\n",
    "\n",
    "          eps_pred=self.cn_unet(sample=noisy_latents, timestep=timesteps, encoder_hidden_states=prompt_embeddings).sample\n",
    "\n",
    "          #train the text prompt embedding s.t. pretrained SD UNet appropriately estimates noise from the prompt embedding and latent from the image\n",
    "          loss=mse_loss(eps_pred, eps).mean([1,2,3]).mean()\n",
    "          inversion_train_logs['loss_history'].append(loss.item())\n",
    "          accelerator.backward(loss)\n",
    "\n",
    "          # Zero out the gradients for all token embeddings except the newly added\n",
    "          # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "          if accelerator.num_processes > 1:\n",
    "            #for multiple GPUs\n",
    "            emb_grads = self.trainable_text_encoder.module.get_input_embeddings().weight.grad\n",
    "            index_grads_to_zero = torch.arange(len(self.tokenizer)) != placeholder_token_id #ids other than placeholder token.\n",
    "            self.trainable_text_encoder.module.get_input_embeddings().weight.grad.data[index_grads_to_zero, :] = emb_grads.data[index_grads_to_zero, :].fill_(0)\n",
    "          else:\n",
    "            emb_grads = self.trainable_text_encoder.get_input_embeddings().weight.grad\n",
    "            index_grads_to_zero = torch.arange(len(self.tokenizer)) != placeholder_token_id #ids other than placeholder token.\n",
    "            self.trainable_text_encoder.get_input_embeddings().weight.grad.data[index_grads_to_zero, :] = emb_grads.data[index_grads_to_zero, :].fill_(0)\n",
    "          #print(\"Step: {:d}\".format(global_step))\n",
    "          #print(emb_grads[self.inversion_dataset.placeholder_token_id])\n",
    "          #check that embedding has gradient\n",
    "          #print(\"Embedding Gradient: mean={:.8f}, std={:.8f}\".format(torch.mean(emb_grads).item(), torch.std(emb_grads).item()))\n",
    "\n",
    "          # Get the index for tokens that we want to zero the grads for\n",
    "\n",
    "          #update only the placeholder token embedding from the token embeddings\n",
    "          optimizer.step()\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "        #progress if gradients sync well\n",
    "        if accelerator.sync_gradients:\n",
    "          pbar.update(1)\n",
    "          global_step += 1\n",
    "          #self.check_embedding_alignment()\n",
    "        if global_step >= max_train_steps:\n",
    "          break\n",
    "    pbar.close()\n",
    "\n",
    "    #plug the placeholder embedding of trainable into actual text_encoder\n",
    "    placeholder_embedding=self.trainable_text_encoder.get_input_embeddings().weight.data[placeholder_token_id]\n",
    "    self.text_encoder.get_input_embeddings().weight.data[placeholder_token_id]=placeholder_embedding\n",
    "\n",
    "    matches=self.check_embedding_alignment()\n",
    "    if matches:\n",
    "      #save tokenizer and text_encoder do hf repository\n",
    "      self.tokenizer.push_to_hub(repo_name)\n",
    "      self.text_encoder.push_to_hub(repo_name)\n",
    "      print(\"Push Success\")\n",
    "\n",
    "    #plot train loss\n",
    "    self.plot_train_logs(inversion_train_logs)\n",
    "    return inversion_train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Images\n",
    "preprocess=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((768,768)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def open_images(learnable_property, name):\n",
    "  img_list=[]\n",
    "  idx=1\n",
    "  while True:\n",
    "    folder=name\n",
    "    img_name=name+\"_{:d}\".format(idx)\n",
    "    dir=os.path.join(\".\", 'images/{:s}/{:s}.jpg'.format(folder, img_name))\n",
    "    if os.path.exists(dir):\n",
    "      img=preprocess(PIL.Image.open(dir)).to(device)\n",
    "      img_list.append(img.unsqueeze(dim=0))\n",
    "      idx+=1\n",
    "    else:\n",
    "      break\n",
    "  images=torch.cat(img_list, dim=0)\n",
    "  return images\n",
    "\n",
    "def show_images(images):\n",
    "  if len(images.size())==3:\n",
    "    images=images.unsqueeze(dim=0)\n",
    "  n_imgs=images.size(0)\n",
    "  n_cols=4\n",
    "  n_rows=math.ceil(n_imgs/n_cols)\n",
    "  width=20\n",
    "  height=5*n_rows\n",
    "  plt.figure(figsize=(width, height))\n",
    "  for idx, image in enumerate(images):\n",
    "    plt.subplot(n_rows, n_cols, idx+1)\n",
    "    plt.imshow(image.permute(1,2,0).detach().cpu().numpy())\n",
    "  plt.show()\n",
    "\n",
    "#test image set\n",
    "illustration_images=open_images(\"style\", \"3d_illustration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inversion Training of 3D style\n",
    "images=illustration_images\n",
    "initial_token=\"illustration\"\n",
    "placeholder_token=\"<3d_illustration>\"\n",
    "learnable_property=\"style\"\n",
    "\n",
    "#2k train steps take about 25min.s w/ A100 GPU\n",
    "#5k steps will take about 60min.s w/ A100 GPU\n",
    "train_logs=inversion_trainer.train(images=images, initial_token=initial_token, placeholder_token=placeholder_token, learnable_property=learnable_property,\n",
    "                                   lr=5e-3, max_train_steps=5000, train_batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference Models\n",
    "pipeline=diffusers.StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n",
    "pipeline.tokenizer=transformers.CLIPTokenizer.from_pretrained('{:s}/{:s}'.format(username, repo_name))\n",
    "pipeline.text_encoder=transformers.CLIPTextModel.from_pretrained('{:s}/{:s}'.format(username, repo_name))\n",
    "\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_attention_slicing()\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "prompt=\"a desk at a office with computers in the style of <3d_illustration>\"\n",
    "\n",
    "image=pipeline(prompt=prompt).images[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "641a7458bfae2bc959d7f867e9e3882167acabe29543290f7c5231fa0d54378e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
